
**1. Robots.txt là gì?
----------------------

Tệp robots.txt là một tệp ở gốc trang web của bạn cho biết những phần thuộc trang web bạn không muốn cấp quyền truy cập cho trình thu thập dữ liệu của công cụ tìm kiếm.

**2. robots.txt được sử dụng vì mục đích gì?
--------------------------------------------

Sử dụng để kiểm soát lưu lượng thu thập dữ liệu, thường là vì bạn không muốn máy chủ của bạn bị quá tải bởi trình thu thập dữ liệu của Google hoặc không muốn lãng phí ngân sách thu thập dữ liệu vào việc thu thập dữ liệu những trang không quan trọng hay giống nhau trên trang web của bạn. Bạn không nên sử dụng robots.txt như một phương tiện để ẩn các trang web của bạn khỏi kết quả tìm kiếm của Google. Điều này là vì các trang khác có thể trỏ đến trang của bạn, và trang của bạn có thể được lập chỉ mục theo cách đó để tránh tệp robots.txt. Nếu bạn muốn chặn trang của bạn khỏi kết quả tìm kiếm, hãy sử dụng một phương pháp khác như bảo vệ bằng mật khẩu hoặc lệnh hoặc thẻ không lập chỉ mục.

**3. những hạn chế của robots.txt:
----------------------------------

- Hướng dẫn trong robots.txt chỉ là lệnh

- Trình thu thập khác nhau phân tích cú pháp khác nhau

- robots.txt của bạn không thể chặn các trang web khác tham chiếu tới URL của bạn

**4. Tạo tệp robots.txt:
------------------------

Rất đơn giản, bạn chỉ cần tạo 1 file tên Robots với phần mở rộng txt mà bất cứ trình soạn thảo nào cũng có thể làm được.

Thư mục chứa Robots.txt là thư mục gốc chứa mã nguồn website của bạn.

húng ta cùng tìm hiểu 4 lệnh cơ bản của Robots.txt bao gồm:

User-agent:`[tên của robot được áp dụng quy tắc sau]`. User-agent của robot mà bạn cần chặn hoặc cấp quyền truy cập, sử dụng * cho tất cả robot.
Disallow:`[đường dẫn URL mà bạn muốn chặn]` . Không cho phép crawler truy cập một nội dung.
Allow:`[đường dẫn URL trong một thư mục con, trong một thư mục gốc bị chặn, mà bạn muốn bỏ chặn]`. Cho phép crawler truy cập một nội dung.
Sitemap: Thông báo cho máy tìm kiếm biết XML Sitemap của trang web.

Các ví dụ cụ thể

Để không cho phép robot nào truy cập nội dung trang web của bạn.
User-agent: *
Disallow: /
Hữu ích trong việc ngăn chặn robot truy cập vào trang quản trị website của bạn.

Để cho phép mọi robot truy cập tất cả nội dung trang web của bạn
User-agent: *
Disallow:
(đơn giản chỉ cần tạo file robots.txt trắng hoặc xóa nó đi là xong)

Để ngăn chặn robot truy cập một vài nội dung trên trang web. Chúng ta sẽ chặn toàn bộ file/ thư mục con trong /cgi-bin/ và /private-directory/
User-agent: *
Disallow: /cgi-bin/
Disallow: /private-directory/

Để ngăn chặn đích danh một robot nào đó truy cập trang web. Chúng ta thử chặn BadBot nhé
User-agent: BadBot
Disallow: /

Để cho phép các robot truy cập một thư mục con của một thư mục bị cấm truy cập. Trong trường hợp này, chúng ta chặn toàn bộ file/ thư mục con của /private-parent/ ngoại trừ /public-child/. Tạm hiểu đây là tính năng ghi đè (override) trong robots.txt nhé
User-agent: *
Disallow: /private-parent/
Allow: /private-parent/public-child/

<img src="http://i.imgur.com/Ozl03YZ.png">